---
title: "Lab 7: Text Mining"
author: "Madeleine Jones"
date: "10/13/2021"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

Congratulations you've successfully transferred from being a NBA 'quant' scout to a consultant specializing in US national sentiment! You've been hired by a non-profit in secret to track the level of support nationally and regionally for the Climate Change issues. The goal is to get a general idea of patterns associated with articles being written on the broad topic of Climate Change (you can also choose to select a sub-topic). In doing so your data science team has decided to explore periodicals from around the country in a effort to track the relative positive or negative sentiment and word frequencies. Luckily you have access to a world class library search engine call LexusNexus (NexusUni) that provides access to newspapers from around the country dating back decades. You'll first need to decided what words you want to track and what time might be interesting to begin your search. 

You'll need to select several newspapers from different regions in the country limiting the search to 100 articles from each paper, run sentiment analysis with each newspaper serving as a corpus and then compare the level of positive or negative connotation associated with the outcomes. Also, work through tf*idf on each corpus (newspapers) and compare the differences between the distributions (5 to 6 newspapers should be fine)

Newspaper is corpus and articles are within
Download as text file, remove intro 

Your main goal (and the goal of all practicing data scientists!) is to translate this information into action. What patterns do you see, why do you believe this to be the case? What additional information might you want? Be as specific as possible, but keep in mind this is an initial exploratory effort...more analysis might be needed...but the result can and should advise the next steps you present to the firm. 


Please submit a cleanly knitted HTML file describing in detail the steps you 
took along the way, the results of your analysis and most importantly the implications/next steps you would recommend. A selected few of you will report your final results and recommendations next week in class (5 minutes per group) 

You will need also need to try to collaborate within your group via a GitHub repo, if you choose it would be fine to assign 1 or 2 regions/newspapers per group member, that can then be added to the repo individually. Create a main repo, everyone should work in this repo and submit independently using forking/pull requests. Select a repo owner that sets up access (read access) for the week, we will rotate owners next week. 
Also, submit a link to your the GitHub repo (every group member can submit the same link). 

Here is the link to the database search via the UVA Library that should lead you to LexusNexus (Now Nexas Uni)
https://guides.lib.virginia.edu/az.php?a=l


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

Chosen Papers: The Philadelphia Inquirer and The Tampa Bay Times 

## Newspaper 1: The Philadelphia Inquirer 
```{r}
library(tidyverse)  # load the required packages
library(tidytext)
library(ggwordcloud)
library(gutenbergr)
library(textdata)
library(DT)
library(patchwork)
```

### Reading in, Cleaning, and Structuring Article Files {.tabset}

```{r}
# reading in all of the txt files
a1 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/A frightening future for butterflies.txt", header = FALSE)
a2 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/A Rising Threat To Water Systems.txt", header = FALSE)
a3 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/Along coast_ climate change demands urgent action.txt", header = FALSE)
a4 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/Climate change response needs carbon pricing.txt", header = FALSE)
a5 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/Climate change should concern pro-lifers.txt", header = FALSE)
a6 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/Conservatives_ but green_ too.txt", header = FALSE)
a7 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/Fall Palette.txt", header = FALSE)
a8 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/How Climate Change Could Affect Wine.txt", header = FALSE)
a9 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/Ida_s fury.txt", header = FALSE)
a10 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/Let_s speak for the trees.txt", header = FALSE)
a11 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/Mosquitos get more biting days.txt", header = FALSE)
a12 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/Pa. chicken farming may rise due to climate change.txt", header = FALSE)
a13 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/Index fund giant takes on climate change.txt", header = FALSE)
a14 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/Teen voices on climate change.txt", header = FALSE)
a15 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/The planet_s other threat.txt", header = FALSE)
a16 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/The price of extreme weather.txt", header = FALSE)
a17 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/Weather service director to retire.txt", header = FALSE)
a18 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Philly Paper/Wharton prof joins Democratic race for Senate.txt", header = FALSE)
```

```{r}
# trimming all txt files to body of article
a1 <- a1[22:48,]
a2 <- a2[22:100,]
a3 <- a3[21:40,]
a4 <- a4[21:46,]
a5 <- a5[21:43,]
a6 <- a6[22:53,]
a7 <- a7[22:40,]
a8 <- a8[22:49,]
a9 <- a9[22:67,]
a10 <- a10[22:41,]
a11 <- a11[22:58,]
a12 <- a12[21:51,]
a13 <- a13[22:66,]
a14 <- a14[21:83,]
a15 <- a15[22:50,]
a16 <- a16[22:50,]
a17 <- a17[22:53,]
a18 <- a18[22:42,]

```

```{r}
articles <- list(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, a16, a17, a18)  # make a list of all of the articles for easier cleaning and processing

combinecols <- function(x){  # function to combine the three columns of each txt file into one text function
  x$text <- paste(x$V1, x$V2, x$V3)  # paste function takes the input columns and "pastes" them into the new text column
  return(data.frame(x$text))  # return just the completed text column
}

articles <- lapply(articles, combinecols)  # apply the function to combine columns to all of the articles 

wordfreq <- function(x){     # function that calculates the frequency of words in an article
  x %>% 
    unnest_tokens(word, x.text)%>%  # unnest_tokens pulls apart the text by word
    anti_join(stop_words)%>%   # remove stop words (words such as the, an, it, etc)
    count(word, sort=TRUE)  # count the occurrences of each word and sort from largest to smallest
}

articles <- lapply(articles, wordfreq)  # apply the word frequency function to each of the articles
```

18 articles from The Philadelphia Inquirer have been selected and downloaded from LexusNexus (Now Nexas Uni).  The articles have been stripped to their body text and the frequencies of words within the text have been summed.  Results for 3 of the articles are shown below:

#### A Frightening Future for Butterflies 
```{r}
print1a <- data.frame(articles[1])  # covert the first article element to a data frame to print
datatable(print1a)
```

#### A Rising Threat to Water Systems 
```{r}
print2a <- data.frame(articles[2])  # covert the second article element to a data frame to print
datatable(print2a)
```

#### Along the Coast, Climate Change Demands Urgent Action
```{r}
print3a <- data.frame(articles[3])  # covert the third article element to a data frame to print
datatable(print3a)
```

### Sentiment Analysis Methods {.tabset}

```{r}
#helps with the sentiment analysis, using package "textdata"

sent_affin <- function(x){  # function to determine the sentiment using AFFIN
  data.frame(x) %>%   
    inner_join(get_sentiments("afinn"))  # join the article words with AFFIN sentiment words based on common words in both
}

sent_affin_list <- lapply(articles, sent_affin)  # apply the AFFIN sentiment function to all of the articles 

sent_nrc <- function(x){  # function to determine the sentiment using NRC
  data.frame(x) %>% 
    inner_join(get_sentiments("nrc"))  # join the article words with NRC sentiment words based on common words in both
}

sent_nrc_list <- lapply(articles, sent_nrc)  # apply the NRC sentiment function to all of the articles 

sent_bing <- function(x){   # function to determine the sentiment using BING
  data.frame(x) %>% 
    inner_join(get_sentiments("bing"))  # join the article words with BING sentiment words based on common words in both
}

sent_bing_list <- lapply(articles, sent_bing)  # apply the BING sentiment function to all of the articles 

```

#### AFINN 
```{r}
affin_plot <- function(index){  # create a plot of the AFFIN sentiment scores 
  ggplot(data = data.frame(sent_affin_list[index]),   # use the AFFIN score from one of the articles
          aes(x=value)  # make the x-axis the sentiment values
          )+
    geom_histogram(bins=6)+  # use a histogram with 11 bins to plot
    ggtitle(paste("Article", index))+   # title each plot with the article number
    theme_bw()  # use the balck and white theme
}

affin_plot_list <- lapply(1:18, affin_plot)  # apply the plot function to each of the article indices 
wrap_plots(affin_plot_list[1:9], ncol=3)  # plot the first 9 histograms
wrap_plots(affin_plot_list[10:18], ncol=3)  # plot the second 9 histograms
```

#### NRC 
```{r}
nrc_table <- function(x){  # function to make a table of the NRC sentiment
  tab <- table(data.frame(sent_nrc_list[x])$sentiment)  # for each of the articles make a data frame of the sentiment values and table their values
}
 
nrc_table_vec <- sapply(1:18, nrc_table)  # apply the NRC table function to each of the article indices

nrc_table_df <- data.frame(t(nrc_table_vec))  # transpose the vector so it is long rather than wide and make it a data frame

row.names(nrc_table_df) <- c("Article 1", "Article 2","Article 3","Article 4","Article 5","Article 6","Article 7","Article 8","Article 9","Article 10","Article 11","Article 12","Article 13","Article 14","Article 15","Article 16","Article 17","Article 18")  # name the rows of the dataframe by article

datatable(nrc_table_df)  # print the NRC table data frame
```


#### Bing
```{r}
bing_table <- function(x){  # function to make a table to the BING sentiment scores
  tab <- table(data.frame(sent_bing_list[x])$sentiment)  # make a data frame of the BING sentiment scores and then table their totals
}

bing_table_vec <- sapply(1:18, bing_table)  # apply the bing table function to each of the article indices

bing_table_df <- data.frame(t(bing_table_vec))  # # transpose the vector so it is long rather than wide and make it a data frame 

row.names(bing_table_df) <- c("Article 1", "Article 2","Article 3","Article 4","Article 5","Article 6","Article 7","Article 8","Article 9","Article 10","Article 11","Article 12","Article 13","Article 14","Article 15","Article 16","Article 17","Article 18")  # make the names of the data frame rows correspond to the articles

datatable(bing_table_df)  # print the bing sentiment outputs 
```

### Sentiment Analysis Discussion

To begin by looking at the AFINN sentiment plots which ranks sentiment of words on a -5 to 5 scale, it appears that all articles contain a decent amount of positive and negative sentiment.  Some articles, such as Article 3, which discusses the urgent action needed to combat the problems of climate change along the coast, are more clustered around negative sentiments with a light skew in the positive direction.  Others, such as Article 8, which discusses the effects of climate change on wine, have more positive sentiments with some negative sentiment skew.  This is expected as different article focuses will involve heavier topics, such as civil responsibility in Article 3, or lighter topics, such as social food and drink in Article 8.  Furthermore, in the NRC table, it appears that all of the articles have senses of anger, anticipation, disgust, fear, joy, etc., with different articles emphasizing different emotions.  For example, Article 14 focuses on teen opinions of climate change and has a larger anger score than each of the other articles, indicating the presence teenage frustration with climate change.  Finally, using the more simplified BING lexicon for positive and negative values, it appears that in general, more articles have greater negative sentiment than positive sentiment.  Of the 18 articles, 13 have greater negative scores, 4 have greater positive scores, and 1 has equal scores for positive and negative sentiment.  

### Term Frequency - Inverse Document Frequency Analysis 

```{r}
word_bags <- list(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, a16, a17, a18)  # create a list of all of the articles

word_bags <- lapply(word_bags, combinecols)  # combine the columns of the articles so that each article only has one column of text

data_prep <- function(x,y,z){  # function to prep the word data by putting into one cell for each article
  i <- as_tibble(t(x))  # transpose so its not long, and is wide in one cell 
  ii <- unite(i,"x.text",y:z,remove = TRUE,sep = "")  # unite/ combine the text into one cell 
}

combine_bags <- function(x){  # function to apply the data prep function to varying row indices
  data_prep(data.frame(word_bags[x]), 'V1', paste('V', nrow(data.frame(word_bags[x])), sep="")) # use the data prep function for the x-th article
}

combined_word_bags <- data.frame(sapply(1:18, combine_bags))  # apply the combine bags function to each of the article indices

article <- c("Article 1", "Article 2","Article 3","Article 4","Article 5","Article 6","Article 7","Article 8","Article 9","Article 10","Article 11","Article 12","Article 13","Article 14","Article 15","Article 16","Article 17","Article 18")  # create vector of article numbers

tf_idf_text <- tibble(article,text=t(tibble(combined_word_bags, .name_repair = "universal")))  # create a tibble with combined word bags data and corresponding article names 

word_count <- tf_idf_text %>%   # counts frequency on individual words
  unnest_tokens(word, text) %>%
  count(article, word, sort = TRUE)

total_words <- word_count %>%    # counts total words in each article
  group_by(article) %>% 
  summarize(total = sum(n))

article_words <- left_join(word_count, total_words)   # combine the data from the word count and total words data sets

article_words <- article_words %>%  # calculate the tf_idf using the bind_tf_idf function : if * idf = tf_idf
  bind_tf_idf(word, article, n)  

sorted_article_words <- article_words[order(article_words$tf_idf, decreasing = TRUE),]  # sort the tf_idf in descending order

datatable(sorted_article_words)  # print the tf_idf data
```

By conducting Term Frequency - Inverse Document Frequency analysis, we can see which terms are most prevalent in articles relative to their prevalence in the overall newspaper.  It appears that the terms with greatest relative prevalence are those related to specific topics or entities not necessarily relevant to the topic of Climate Change in general.  For example, Article 8 discusses the effects of climate change on wine, a specific topic not generally connected to climate change discussions, so terms that stand out are "wine" and "varieties."  Further, Article 13 highlights the efforts of index fund companies to combat climate change, so terms such as "Vanguard" and "Blackrock," index fund leaders, and "index" and "fund" stand out.   

## Newspaper 2: The Tampa Bay Times

### Reading in, Cleaning, and Structuring Article Files {.tabset}

```{r}
# reading in all of the txt files
b1 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/As extreme weather increases_ climate misinformation ad.txt", header = FALSE)
b2 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/Biden promises _relentless diplomacy_ to skeptical alli.txt", header = FALSE)
b3 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/Biden says budget talks hit _stalemate__ _3.5T may take.txt", header = FALSE)
b4 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/Biden_ world leaders try to hammer out next steps to co.txt", header = FALSE)
b5 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/Climate activist Nakate seeks immediate action from wor.txt", header = FALSE)
b6 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/Climate change is a national security threat and econom.txt", header = FALSE)
b7 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/Climate change is also on the ballot in Clearwater_s el.txt", header = FALSE)
b8 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/Compare St. Petersburg mayoral candidates Ken Welch and.txt", header = FALSE)
b9 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/Congress takes aim at climate change in massive relief.txt", header = FALSE)
b10 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/Florida Legislature can do even more to make us resilie.txt", header = FALSE)
b11 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/Here_s how climate change could make future Red Tide bl.txt", header = FALSE)
b12 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/Himalayan glacier disaster highlights climate change ri.txt", header = FALSE)
b13 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/How Florida ranchers can help fight climate change _ Co.txt", header = FALSE)
b14 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/In St. Petersburg_ climate change smells like rotting f.txt", header = FALSE)
b15 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/On climate change_ are we too stupid to live__ Columnis.txt", header = FALSE)
b16 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/Republicans should listen on climate change _ Palm Beac.txt", header = FALSE)
b17 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/Study_ Northwest heat wave impossible without climate c.txt", header = FALSE)
b18 <- read.csv("/Users/mj/Desktop/2021 Fall/DS 3001/DS-3001/07_text_mining/Tampa Paper/What_s next for climate change in Florida Legislature__.txt", header = FALSE)
```

```{r}
# trimming all txt files to body of article
b1 <- b1[21:64,]
b2 <- b2[21:48,]
b3 <- b3[21:43,]
b4 <- b4[21:65,]
b5 <- b5[21:32,]
b6 <- b6[21:44,]
b7 <- b7[22:84,]
b8 <- b8[21:81,]
b9 <- b9[21:40,]
b10 <- b10[21:39,]
b11 <- b11[21:78,]
b12 <- b12[21:46,]
b13 <- b13[21:41,]
b14 <- b14[21:34,]
b15 <- b15[21:45,]
b16 <- b16[20:43,]
b17 <- b17[21:41,]
b18 <- b18[21:74,]

```

```{r}
articlesb <- list(b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b12, b13, b14, b15, b16, b17, b18)  # make a list of all of the articles for easier cleaning and processing

articlesb <- lapply(articlesb, combinecols)  # apply the function to combine columns to all of the articles

articlesb <- lapply(articlesb, wordfreq)  # apply the word frequency function to each of the articles to count word occurrences

```

18 articles from the Tampa Bay Times have been selected and downloaded from LexusNexus (Now Nexas Uni).  The articles have been stripped to their body text and the frequencies of words within the text have been summed.  Results for 3 of the articles are shown below:

#### As Extreme Weather Increases, Climate Misinformation Adapts
```{r}
print1b <- data.frame(articlesb[1])  # covert the first article element to a data frame to print
datatable(print1b)
```

#### World Leaders Try to Hammer out Next Steps to Combat Climate Change
```{r}
print2b <- data.frame(articlesb[2])  # covert the second article element to a data frame to print
datatable(print2b)
```

#### Climate Activist Nakate Seeks Immediate Action from World Leaders in Glasgow
```{r}
print3b <- data.frame(articlesb[3])  # covert the third article element to a data frame to print
datatable(print3b)
```

### Sentiment Analysis Methods {.tabset}

```{r}
#helps with the sentiment analysis, using package "textdata"

sent_affin_listb <- lapply(articlesb, sent_affin)  # apply the function to determine AFFIN sentiment to each of the articles

sent_nrc_listb <- lapply(articlesb, sent_nrc)  # apply the function to determine NRC sentiment to each of the articles

sent_bing_listb <- lapply(articlesb, sent_bing)  # apply the function to determine BING sentiment to each of the articles

```

#### AFINN 
```{r}
affin_plotb <- function(index){  # create a plot of the AFFIN sentiment scores 
  ggplot(data = data.frame(sent_affin_listb[index]),   # use the AFFIN score from one of the articles
          aes(x=value)  # make the x-axis the sentiment values
          )+
    geom_histogram(bins=6)+  # use a histogram with 11 bins to plot
    ggtitle(paste("Article", index))+   # title each plot with the article number
    theme_bw()  # use the black and white theme
}

affin_plot_listb <- lapply(1:18, affin_plotb)   # apply the plot function to each of the article indices 
wrap_plots(affin_plot_listb[1:9], ncol=3)   # plot the first 9 histograms
wrap_plots(affin_plot_listb[10:18], ncol=3)   # plot the second 9 histograms

```

#### NRC 
```{r}
nrc_tableb <- function(x){  # function to make a table of the NRC sentiment
  tab <- table(data.frame(sent_nrc_listb[x])$sentiment)# for each of the articles make a data frame of the sentiment values and table their values
}

nrc_table_vecb <- sapply(1:18, nrc_tableb)  # apply the NRC table function to each of the article indices
 
nrc_table_dfb <- data.frame(t(nrc_table_vecb))  # transpose the vector so it is long rather than wide and make it a data frame

row.names(nrc_table_dfb) <- c("Article 1", "Article 2","Article 3","Article 4","Article 5","Article 6","Article 7","Article 8","Article 9","Article 10","Article 11","Article 12","Article 13","Article 14","Article 15","Article 16","Article 17","Article 18")  # name the rows of the data frame by article

datatable(nrc_table_dfb)   # print the NRC table data frame

```


#### Bing
```{r fig.show}
bing_tableb <- function(x){  # function to make a table of the BING sentiment
  tab <- table(data.frame(sent_bing_listb[x])$sentiment)  # for each of the articles make a data frame of the sentiment values and table their values
}

bing_table_vecb <- sapply(1:18, bing_tableb)  # apply the BING table function to each of the article indices

bing_table_dfb <- data.frame(t(bing_table_vecb))  # transpose the vector so it is long rather than wide and make it a data frame

row.names(bing_table_dfb) <- c("Article 1", "Article 2","Article 3","Article 4","Article 5","Article 6","Article 7","Article 8","Article 9","Article 10","Article 11","Article 12","Article 13","Article 14","Article 15","Article 16","Article 17","Article 18")  # name the rows of the data frame by article

datatable(bing_table_dfb) # print the BING table data frame


```

### Sentiment Analysis Discussion

First looking at the AFINN sentiment distributions, it appears that all of the articles contain a mix of positively and negatively connotative words. Some articles are more positive with a skew towards negative sentiment such as Article 8.  This article discusses mayoral candidates in a more factual, less bias manner contributing to the relatively positive sentiment.  On the other hand, some articles are more negative with a skew towards positive sentiment such as Article 12 which discusses the risks of climate change after a Himalayan glacier disaster.  These varying sentiment distributions highlight the many topics that are related to and affected by climate change, along with the differing tones used to describe each topic.  Moving to the NRC sentiment breakdown, each of the articles also has a unique distribution among emotional sentiment categories such as anger, fear, joy, and trust.  It appears from this breakdown, most articles have high numbers for positive sentiment in comparison to the other categories.  However, this result of high positive sentiments seen with the NRC lexicon is not present when using the BING lexicon.  In the BING assessment, only 8 of the 18 articles have higher positive scores than negative scores, while the majority gave greater negative scores. These varying conclusions of positive and negative sentiment can be attributed to the different methodologies behind the lexicons and should be investigated further with more articles to gain a better sense of public sentiment.  

### Term Frequency - Inverse Document Frequency Analysis

```{r}
word_bagsb <- list(b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b12, b13, b14, b15, b16, b17, b18) # create a list of all of the articles

word_bagsb <- lapply(word_bagsb, combinecols)  # combine the columns of the articles so that each article only has one column of text

bagb <- data_prep(data.frame(word_bagsb[1]), 'V1', paste('V', nrow(data.frame(word_bagsb[1])), sep=""))

combine_bagsb <- function(x){   # function to apply the data prep function to varying row indices
  data_prep(data.frame(word_bagsb[x]), 'V1', paste('V', nrow(data.frame(word_bagsb[x])), sep=""))  # use the data prep function for the x-th article
}

combined_word_bagsb <- data.frame(sapply(1:18, combine_bagsb))   # apply the combine bags function to each of the article indices

article <- c("Article 1", "Article 2","Article 3","Article 4","Article 5","Article 6","Article 7","Article 8","Article 9","Article 10","Article 11","Article 12","Article 13","Article 14","Article 15","Article 16","Article 17","Article 18")  # create vector of article numbers

tf_idf_textb <- tibble(article,text=t(tibble(combined_word_bagsb, .name_repair = "universal"))) # create a tibble with combined word bags data and corresponding article names 

word_countb <- tf_idf_textb %>%   # counts frequency on individual words
  unnest_tokens(word, text) %>%
  count(article, word, sort = TRUE)

total_wordsb <- word_countb %>%    # counts total words in each document by president 
  group_by(article) %>% 
  summarize(total = sum(n))

article_wordsb <- left_join(word_countb, total_wordsb)  # combine the data from the word count and total words data sets


article_wordsb <- article_wordsb %>%  # calculate the tf_idf using the bind_tf_idf function : if * idf = tf_idf
  bind_tf_idf(word, article, n)

sorted_article_wordsb <- article_wordsb[order(article_wordsb$tf_idf, decreasing = TRUE),]  # sort the tf_idf in descending order

datatable(sorted_article_wordsb)  # print the tf_idf data

```

Using the Term Frequency - Inverse Document Frequency analysis, we can determine which terms are most prevalent in articles relative to their overall prevalence in the newspaper.  The results depict that terms which are most prevalent to certain articles are those that are specific to a person, place, or thing that is not necessarily relevant to climate change in a general sense.  For example, Article 14 discusses the death of marine life caused by climate change rather than tropical storm Elsa, so its stand out term is "Elsa."  Similarly, Article 5 discusses the efforts of a climate change activist Venessa Nakate during a UN conference, so its stand out term is "Nakate."

## Patterns and Next Steps 

Overall, there is a variety of more positively and more negatively-sentimented articles.  It appears that the overall trend leans more negative, which is not surprising given that climate change itself is a negatively connotative phenomena.  Many of the articles discuss a negative impact of climate change such as a decrease in species health or an increase in economic problems.  Articles that are not directly focused on a negative impact are typically centered around politics and have a relatively neutral tone, when focusing on the facts, or a polarizing positive or negative tone when discussing a particular candidate.  There do appear to be a subset of articles that lean more positive in sentiment, such as Article 13 from the Philadelphia Inquirer, that are typically written to discuss efforts to better the climate change issue.  While these articles have tones of anticipation and excitement, there is also some negativity when explaining the current state and difficulty in combating current climate trends.  

To continue the research process in understanding sentiment regarding climate change, I would suggest looking into articles categorized by field of interest.  Through initial analysis, it has become apparent that climate change plays a role in many different fields, from food and drink industries, to wildlife activism, to political settings.  With such a broad analysis, these different fields can make accurate and specific conclusions challenging, often leading to over-generalizations of public sentiment.  A breakdown analysis on the other hand will allow for sentiment to be understood more concretely at the field level, and its results can be used to analyze sentiment on a article by article basis.  This more detailed analyses will require additional articles from each newspaper as well as efforts to categorize these articles into more nuclear fields.  Once newspaper articles are analyzed by category, we will gain a more concrete idea of public sentiment regarding climate change specific to different industries with unique values.  

